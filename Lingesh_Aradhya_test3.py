#!/usr/bin/python
# -*- coding: utf-8 -*-

'''
    Created By - Lingesh Aradhya
    License: GNU GPL
    Source Control: https://github.com/Llivingon/
    Creation Date: 12 Mar 2019
  Overview -
    Platform - Unix/Windows
    Use Open source scripting language - Python
    Create an api that takes a string as input and returns a list of text insights from the top 5 google results of the query generated by those keywords.
    Test with the following string « keystone - Circular reference found role inference » . You will need to build a query , search the web (you can scrape google) ,
    select the top 5 results and for each result scrape for the most relevant information on that page (relevant from a user’s perspective based on the original
    string ) . The response should contain the links to the websites that were scraped with the most relevant information (could be a text insight ,
    a code snippet, a patch file, etc.) for each. In the response, feel free to add any other extra information that you think could be relevant.

  TODO- Build a Flask API from below method.
        Refactor code to inster results into Nosql DB instead of writing to a StringIO / File
        
  Background
    Install Python 2.7
    Install below modules
'''

from bs4 import BeautifulSoup
from bs4.element import Comment
from googlesearch import search
from Queue import Queue
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from StringIO import StringIO
from threading import Thread

import nltk
import shutil
import requests
import re
import uuid
import unittest

nltk.download('punkt')

QUEUE_VAR = Queue(maxsize=0)
TOTAL_WEBS_TO_SCRAPE = 5
STOP_WORDS = set(stopwords.words('english'))

class Assignment3(object):
    """returns a list of text insights from the top 5 google results
    """

    def __init__(self):
        """"""
        print "Will look for insights on %d wesbites"%TOTAL_WEBS_TO_SCRAPE
        self.output  = StringIO()
    
    def _googleSearch(self, searchString, tld="com", num=TOTAL_WEBS_TO_SCRAPE):
        resultUrlsList = []
        for i in search(searchString, lang='en', tld=tld, num=TOTAL_WEBS_TO_SCRAPE, stop=TOTAL_WEBS_TO_SCRAPE, pause=1): 
            resultUrlsList.append(i)
        #URLs returned by Google search
        return resultUrlsList

    def textInsights(self, searchString):
        print "Search String:"+searchString
        fileName = uuid.uuid4().hex
        tokens = self._tokenizeSearchString(searchString)
        urls = self._googleSearch(searchString)
        for url in urls:
            QUEUE_VAR.put((url))
        try:
            for url in urls:
              worker = Thread(target=self._webScraper, args=(url, tokens, QUEUE_VAR,))
              worker.setDaemon(True)
              worker.start()
            QUEUE_VAR.join()
            with open (fileName, 'w') as fd:
              self.output.seek(0)
              shutil.copyfileobj (self.output, fd)
            print "Test insights for %s on %s"%(searchString ,fileName)
        except:
            print "Something went wrong"

    def _tokenizeSearchString(self, searchString):
        """Split search string into meaning words. These words will be used to identify meaningful texts within html response"""
        words = word_tokenize(searchString)
        wordsFiltered = []
        for w in words:
            if w not in STOP_WORDS:
                wordsFiltered.append(w)
        processedWords = set(wordsFiltered)
        nonAlNumsRemoved = []
        for i in processedWords:
            if i.isalnum():
                nonAlNumsRemoved.append(i)
        print "List of processed tokens:" + str(nonAlNumsRemoved)
        return nonAlNumsRemoved

    def _webScraper(self, url, tokens, QUEUE_VAR):
        """Identify Text insights based on tokens, this method is run in parallel threads"""
        """TBD - Need to update code to use DB instead of writing to StringIO"""
        def _visible_texts(soup, tokens):
            """ Select only visible text from a html """
            RE_SPACES = re.compile(r'\s{3,}')
            text = ' '.join([
                s for s in soup.strings
                if s.parent.name not in ('style', 'script', 'head', 'title', 'meta', '[document]')
                if any(x in s for x in tokens) 
            ])
            # collapse multiple spaces to two spaces.
            return RE_SPACES.sub('  ', text)
    
        while True:
            try:
                url = status_code = title = ""
                urlList = visible_texts = []
                url = QUEUE_VAR.get()
                self.output.write("*********")
                self.output.write("\nUrl:"+url)
                page = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=5)
                status_code = str(page.status_code)
                self.output.write(title)
                self.output.write("\nTitle:"+title)                
                soup = BeautifulSoup(page.text, 'lxml')
                title = soup.title.string
                urlList = []
                for a in soup.find_all('a', href=True):
                    if (("http" in a['href']) and True):
                        urlList.append(a['href'])
                self.output.write("\nURLs scraped on %s:\n"%url)
                self.output.write(unicode(str(urlList), "utf-8"))

                # Selecting Relevant Text Content - All content that contain input string (split into relevant tokens)
                visible_texts = _visible_texts(soup, tokens)

                #Cleaning up non-ascii characters
                print_texts = ''.join([i if ord(i) < 128 else ' ' for i in visible_texts])
                self.output.write("\nText Insights on:%s\n%s"%(url, unicode(str(print_texts), "utf-8")))
            except:
                print("Unable to Scrape data on Url"+url)
            finally:
                QUEUE_VAR.task_done()
        return True


class SimpleTestCase(unittest.TestCase):

    def setUp(self):
        """Call before every test case."""
        self.obj = Assignment3()

    def tearDown(self):
        """Call after every test case."""
        pass

    def test1(self):
        self.obj.textInsights('keystone - Circular reference found role inference')     

if __name__ == '__main__':
    unittest.main()    
